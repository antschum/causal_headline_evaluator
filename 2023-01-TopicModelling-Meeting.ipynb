{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a712caa8",
   "metadata": {},
   "source": [
    "2023-12-12 Meeting\n",
    "\n",
    "- Use click rate everywhere\n",
    "- Implement Causal Model: + Reading\n",
    "    - We divide every click rate by the article mean and the plug it in regression\n",
    "- Make Regplot https://seaborn.pydata.org/generated/seaborn.regplot.html - should expect a clear upwards trend\n",
    "- Taking the predictions of the correlational model and compare the top and least rated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a417d79a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.ufunc size changed, may indicate binary incompatibility. Expected 232 from C header, got 216 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\causalclicker\\lib\\site-packages\\sentence_transformers\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.2.2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence-transformers\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentencesDataset, ParallelSentencesDataset\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLoggingHandler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceTransformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\causalclicker\\lib\\site-packages\\sentence_transformers\\datasets\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDenoisingAutoEncoderDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DenoisingAutoEncoderDataset\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mNoDuplicatesDataLoader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NoDuplicatesDataLoader\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mParallelSentencesDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\causalclicker\\lib\\site-packages\\sentence_transformers\\datasets\\DenoisingAutoEncoderDataset.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mInputExample\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputExample\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtreebank\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TreebankWordDetokenizer\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDenoisingAutoEncoderDataset\u001b[39;00m(Dataset):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\causalclicker\\lib\\site-packages\\nltk\\__init__.py:146\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjsontags\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# PACKAGES\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\causalclicker\\lib\\site-packages\\nltk\\chunk\\__init__.py:155\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Natural Language Toolkit: Chunkers\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright (C) 2001-2023 NLTK Project\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# For license information, see LICENSE.TXT\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mClasses and interfaces for identifying non-overlapping linguistic\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03mgroups (such as base noun phrases) in unrestricted text.  This task is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;03m     pattern is valid.\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChunkParserI\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregexp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RegexpChunkParser, RegexpParser\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    158\u001b[0m     ChunkScore,\n\u001b[0;32m    159\u001b[0m     accuracy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m     tree2conlltags,\n\u001b[0;32m    166\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\causalclicker\\lib\\site-packages\\nltk\\chunk\\api.py:13\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Natural Language Toolkit: Chunk parsing API\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright (C) 2001-2023 NLTK Project\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m##  Chunk Parser Interface\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m##//////////////////////////////////////////////////////\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChunkScore\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParserI\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\causalclicker\\lib\\site-packages\\nltk\\chunk\\util.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy \u001b[38;5;28;01mas\u001b[39;00m _accuracy\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmapping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m map_tag\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m str2tuple\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tree\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\causalclicker\\lib\\site-packages\\nltk\\tag\\__init__.py:70\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TaggerI\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m str2tuple, tuple2str, untag\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     71\u001b[0m     SequentialBackoffTagger,\n\u001b[0;32m     72\u001b[0m     ContextTagger,\n\u001b[0;32m     73\u001b[0m     DefaultTagger,\n\u001b[0;32m     74\u001b[0m     NgramTagger,\n\u001b[0;32m     75\u001b[0m     UnigramTagger,\n\u001b[0;32m     76\u001b[0m     BigramTagger,\n\u001b[0;32m     77\u001b[0m     TrigramTagger,\n\u001b[0;32m     78\u001b[0m     AffixTagger,\n\u001b[0;32m     79\u001b[0m     RegexpTagger,\n\u001b[0;32m     80\u001b[0m     ClassifierBasedTagger,\n\u001b[0;32m     81\u001b[0m     ClassifierBasedPOSTagger,\n\u001b[0;32m     82\u001b[0m )\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbrill\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BrillTagger\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbrill_trainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BrillTaggerTrainer\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\causalclicker\\lib\\site-packages\\nltk\\tag\\sequential.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Tuple\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m jsontags\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NaiveBayesClassifier\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConditionalFreqDist\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FeaturesetTaggerI, TaggerI\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\causalclicker\\lib\\site-packages\\nltk\\classify\\__init__.py:97\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpositivenaivebayes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PositiveNaiveBayesClassifier\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrte_classify\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RTEFeatureExtractor, rte_classifier, rte_features\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscikitlearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SklearnClassifier\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msenna\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Senna\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtextcat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextCat\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\causalclicker\\lib\\site-packages\\nltk\\classify\\scikitlearn.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DictionaryProbDist\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DictVectorizer\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\causalclicker\\lib\\site-packages\\sklearn\\__init__.py:83\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     80\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     81\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     )\n\u001b[1;32m---> 83\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[0;32m     86\u001b[0m     __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    130\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\causalclicker\\lib\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _IS_32BIT\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\causalclicker\\lib\\site-packages\\sklearn\\utils\\__init__.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiscovery\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m all_estimators\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_version, threadpool_info\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmurmurhash\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m murmurhash3_32\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     29\u001b[0m     _is_arraylike_not_scalar,\n\u001b[0;32m     30\u001b[0m     as_float_array,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     indexable,\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Do not deprecate parallel_backend and register_parallel_backend as they are\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# needed to tune `scikit-learn` behavior and have different effect if called\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# from the vendored version or or the site-package version. The other are\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# utilities that are independent of scikit-learn so they are not part of\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# scikit-learn public API.\u001b[39;00m\n",
      "File \u001b[1;32msklearn\\utils\\murmurhash.pyx:1\u001b[0m, in \u001b[0;36minit sklearn.utils.murmurhash\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.ufunc size changed, may indicate binary incompatibility. Expected 232 from C header, got 216 from PyObject"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeCV, LogisticRegression, LinearRegression\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import csv\n",
    "import torch \n",
    "import pickle\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import spearmanr\n",
    "import math\n",
    "import shap\n",
    "#import langid\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0395e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cpu/gpu\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Set random seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b7888d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load data\n",
    "df = pd.read_csv(\"upworthy-archive-confirmatory-packages-03.12.2020.csv\", low_memory=False)\n",
    "#adding index\n",
    "df.reset_index(inplace=True,names=[\"embedding_id\"])\n",
    "\n",
    "#remove rows without eyecatcher_id (about 100)\n",
    "has_eyecatcher_id = df['eyecatcher_id'].notna()\n",
    "df = df.loc[has_eyecatcher_id]\n",
    "#Create a new column for clickrate\n",
    "df[\"clickrate\"] = round((df[\"clicks\"]/ df[\"impressions\"]), ndigits=10)\n",
    "\n",
    "#filter data based on same clickability_id and eyecatcher_id\n",
    "df['headline_count'] = df.groupby(['clickability_test_id', 'eyecatcher_id']).headline.transform('count')\n",
    "df.columns\n",
    "# filter for all headlines with at least 2 pairs. \n",
    "df = df.loc[df['headline_count']>=2, ['clickability_test_id', 'excerpt', 'headline', 'lede', 'eyecatcher_id', 'clicks', 'headline_count',\"embedding_id\",\"clickrate\",\"impressions\"]]\n",
    "\n",
    "# drop all rows with same headline, clickability_test_id and eyecatcher_id\n",
    "\n",
    "#df = df.drop_duplicates(subset=[\"headline\",\"clickability_test_id\",\"eyecatcher_id\"],keep=False)\n",
    "\n",
    "cti = df[df.duplicated(subset=[\"headline\",\"clickability_test_id\",\"eyecatcher_id\"],keep=False)].clickability_test_id\n",
    "eti = df[df.duplicated(subset=[\"headline\",\"clickability_test_id\",\"eyecatcher_id\"],keep=False)].eyecatcher_id\n",
    "print(df.shape)\n",
    "print(\"we removed: \",((df['clickability_test_id'].isin(cti) & df['eyecatcher_id'].isin(eti))).sum())\n",
    "print(df[((df['clickability_test_id'].isin(cti) & df['eyecatcher_id'].isin(eti)))][['clickability_test_id', 'eyecatcher_id', 'headline']][:20])\n",
    "df = df[~(df['clickability_test_id'].isin(cti) & df['eyecatcher_id'].isin(eti))]\n",
    "print(df.shape)\n",
    "\n",
    "#checking if it was successful\n",
    "# -> could there be cases where there are duplicates and we only delete the headlines and not the whole experiment?\n",
    "print(df[df[\"clickability_test_id\"] == \"546de9399ad54eca4800003c\"]) #this is an example of matching headline, clickability test id and eyecatcher id\n",
    "df = df.sort_values(by='headline_count', ascending=False)\n",
    "#print(df.head())\n",
    "#checking if there are any duplicates\n",
    "#df = df.drop_duplicates(subset=[\"headline\"]) ##this was before\n",
    "##this is a new version\n",
    "dupl_headline = df[df.duplicated(subset=[\"headline\"])] \n",
    "## I believe we are removing too many values here - one of the duplicates we want to keep right?\n",
    "\n",
    "#duplicated = df[dupl_headline]\n",
    "ids = dupl_headline[\"clickability_test_id\"]\n",
    "eid = dupl_headline[\"eyecatcher_id\"]\n",
    "\n",
    "\n",
    "mask = df['clickability_test_id'].isin(ids)&df['eyecatcher_id'].isin(eid)\n",
    "df = df[~mask]\n",
    "print(df.shape) #too few observations :(\n",
    "#print(df[df[\"clickability_test_id\"] == \"545181f8763e26efef000001\"]) #those are staying because there are a lot of observations\n",
    "#print(df[df[\"clickability_test_id\"] == \"54518b6da54be28ef000000b\"]) #those are not staying because there a less packages within the test than then in the one above\n",
    "\n",
    "# Capitalize all words in headline. (we do not have to regenerate embeddings because it does not care) \n",
    "df.headline = df.headline.str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8406f0d9-d5cb-4a41-9ee2-0ef7f7a8f7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we remove all cases within experiment. \n",
    "df[(df['clickability_test_id']=='54ad5a2e65343000152d0000')\n",
    "&(df['eyecatcher_id']=='54ad4e7b653430001e1c0000')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba46a9b-bd0b-4b68-98da-d81df4eec9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that there are no more duplicate headlines. \n",
    "len(df.headline.drop_duplicates()) == len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4a326a-a46e-4cc2-9d8f-c74b96682869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for spanish headlines\n",
    "\n",
    "#headlines = df['headline'].astype(str).tolist()\n",
    "#results = [langid.classify(headline) for headline in headlines]\n",
    "#log_file = \"output_log.txt\"\n",
    "\n",
    "#with open(log_file, \"w\", encoding=\"utf-8\") as log:\n",
    "#    for headline,result in zip(headlines,results):\n",
    "#        language = result\n",
    "       \n",
    "#        log.write(f\"Sentence: {headline}, Identified Language: {language}\\n\")\n",
    "#doesnt work amazingly for some reason and it just outputs all the sentences. The spanish ones are:\n",
    "#Como Decir Todo … Sin Pronunciar Ninguna Palabra\n",
    "#Ve La Protesta Que Todos Deben Conocer, Pero Que Nadie Puede Oir\n",
    "#En Vez De ‘Sí Se Puede,’ Ya Es ‘Sí Se Shhhhhhhhh’?\n",
    "#¿Cómo Se Dice ‘Nada’ En Español?\n",
    "#all of them have the same clickability_id, so I would just remove them, because they are basically one experiment\n",
    "print(df.shape)\n",
    "df = df[df[\"clickability_test_id\"] != \"51436075220cb800020007b3\"]\n",
    "#checking if i dropped exactly 4\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e9301b-1bc9-439c-9284-fdb14133e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tensor\n",
    "clickrate = torch.tensor(df.clickrate.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e091a221-d671-4582-9799-a15d5a8870c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.groupby([\"clickability_test_id\"]).count().mean()) #average of 3.5 packages within one test\n",
    "print(df.groupby([\"clickability_test_id\",\"eyecatcher_id\"]).count().mean()) #average of 3.5 packages with the same eyecatcher id and same clickability_test_id\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137ce53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load prerun embeddings of all-mpnet-base-v2\n",
    "with open('duplicates_removed_embeddings.pkl', \"rb\") as fIn:\n",
    "    stored_data = pickle.load(fIn)\n",
    "    stored_sentences = stored_data['headlines']\n",
    "    stored_embeddings = stored_data['embeddings']\n",
    "#embeddings = [stored_embeddings[stored_sentences.index.get_loc(headline)] for headline in df.headline]\n",
    "#make sure we are getting the right embeddings: \n",
    "\n",
    "    \n",
    "#remove rows without eyecatcher_id\n",
    "#stored_sentences = stored_sentences[has_eyecatcher_id]\n",
    "#stored_embeddings = stored_embeddings[has_eyecatcher_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f76e17",
   "metadata": {},
   "source": [
    "## 1.1 Predicting clickrate from headline embeddings with Ridge regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e082b375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "print(stored_embeddings.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(stored_embeddings, clickrate, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8912d9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Model\n",
    "ridge_model =RidgeCV(alphas=[0.001,0.002,0.005,0.01,0.05,0.07,0.2,0.4,0.6, 1, 10],store_cv_values=True)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "ridge_model.score(X_train,y_train)\n",
    "predictions = ridge_model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, predictions)\n",
    "print(\"Ridge Regression MSE for click difference:\", rmse)\n",
    "print(\"Ridge Regression R2 for click difference:\", r2_score(y_true=y_test, y_pred=predictions))\n",
    "df[\"predictions_ridge\"] = ridge_model.predict(stored_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f807f1",
   "metadata": {},
   "source": [
    "## 1.2 Predicting clickrate from headline embeddings with Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9503b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Model\n",
    "linear_model =LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "linear_model.score(X_train,y_train)\n",
    "predictions = linear_model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "print(\"Linear Regression MSE for clicks:\", rmse)\n",
    "print(\"Linear Regression R2 for clicks:\", r2_score(y_true=y_test, y_pred=predictions))\n",
    "df[\"predictions_linear\"] = linear_model.predict(stored_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021623a7",
   "metadata": {},
   "source": [
    "## 1.3 Visualizing predicted clicks vs actual clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5e5243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\n",
    "# print stuff. \n",
    "predictions = ridge_model.predict(stored_embeddings)\n",
    "print(predictions.max())\n",
    "# visualize real and predicted values\n",
    "fig, ax = plt.subplots()\n",
    "sns.regplot(x = predictions, y = df.clickrate, ax=ax)\n",
    "ax.set(ylabel = 'true clickrate', xlabel = 'predicted clickrate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1711e851",
   "metadata": {},
   "source": [
    "## 2.Causal Model \n",
    "\n",
    "Implement Causal Model: + Reading\n",
    "    - We divide every click rate by the article mean and the plug it in regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73398aa-4ade-4412-ab41-d2e25ea0e1ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calculating mean per clickability_test_id and eyecatcher_id\n",
    "df[\"means\"] = df.groupby([\"clickability_test_id\",\"eyecatcher_id\"])[\"clickrate\"].transform(\"mean\")\n",
    "df.columns\n",
    "df[\"adjusted_clickrate\"] = df['clickrate']-df['means']\n",
    "# set rows with 0 clicks -> adjusted clickrates NA - set to zero\n",
    "#df[\"adjusted_clickrate\"] = df[\"adjusted_clickrate\"].fillna(0)\n",
    "print(df[\"adjusted_clickrate\"].isna().sum())\n",
    "adjusted_clickrate = torch.tensor(df.adjusted_clickrate.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67466e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do we do with headlines that did not get any clicks? \n",
    "# Will they just have an adjusted clickrate of zero?\n",
    "df.loc[(df[\"clickrate\"]/df[\"means\"]).isna(), ['headline', 'eyecatcher_id', 'clicks', 'impressions', 'means', 'clickrate']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0254cb-af0d-4d64-93c0-3106c0152430",
   "metadata": {},
   "source": [
    "## 2.1 Causal model with Ridge Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4c0990-b442-4b5f-b011-871f731f8194",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(stored_embeddings, adjusted_clickrate, test_size=0.2)\n",
    "causal_ridge_model =RidgeCV(alphas=[0.001,0.002,0.005,0.01,0.05,0.07,0.2,0.4,0.6, 1, 10],store_cv_values=True)\n",
    "causal_ridge_model.fit(X_train, y_train)\n",
    "causal_ridge_model.score(X_train,y_train)\n",
    "causal_predictions_rg = causal_ridge_model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, causal_predictions_rg)\n",
    "print(\"Causal Ridge Regression MSE for click difference:\", rmse)\n",
    "print(\"Causal Ridge Regression R2 for click difference:\", r2_score(y_true=y_test, y_pred=causal_predictions_rg))\n",
    "df[\"causal_predictions_ridge\"] = causal_ridge_model.predict(stored_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17d5407-c759-4ea3-acd3-eb8afae04ee2",
   "metadata": {},
   "source": [
    "## 2.2 Causal model with Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7635f29-4864-4141-a142-9026c06ba913",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_linear_model =LinearRegression()\n",
    "causal_linear_model.fit(X_train, y_train)\n",
    "causal_linear_model.score(X_train,y_train)\n",
    "causal_predictions_lm = causal_linear_model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, causal_predictions_lm, squared=False)\n",
    "print(\"Causal Linear Regression MSE for clicks:\", rmse)\n",
    "print(\"Causal Linear Regression R2 for clicks:\", r2_score(y_true=y_test, y_pred=causal_predictions_lm))\n",
    "df[\"causal_predictions_linear\"] = causal_linear_model.predict(stored_embeddings)\n",
    "sns.histplot(df[\"causal_predictions_linear\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759f696b",
   "metadata": {},
   "source": [
    "## 3.1 Compare top / bottom 20 causal model with correlational model - Ridge Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dd3c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets, Layout\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "last20_pred = df.sort_values([\"predictions_ridge\"],ascending=True).loc[:,['headline']][:20]\n",
    "last20_pred_causal = df.sort_values([\"causal_predictions_ridge\"],ascending=True).loc[:,['headline']][:20]\n",
    "widget1 = widgets.Output()\n",
    "widget2 = widgets.Output()\n",
    "\n",
    "# render in output widgets\n",
    "with widget1:\n",
    "    display.display(last20_pred.style.set_caption('Last 20 Ridge'))\n",
    "    last20_pred.info()\n",
    "with widget2:\n",
    "    display.display(last20_pred_causal.style.set_caption('Last 20 Causal'))\n",
    "    last20_pred_causal.info()\n",
    "\n",
    "\n",
    "# add some CSS styles to distribute free space\n",
    "box_layout = Layout(display='flex',\n",
    "                    flex_flow='row',\n",
    "                    justify_content='space-around',\n",
    "                    width='auto'\n",
    "                   )\n",
    "    \n",
    "\n",
    "box = widgets.HBox([widget1, widget2], layout=box_layout)\n",
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492a61f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for intersection\n",
    "np.intersect1d(last20_pred.values,last20_pred_causal.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321c94a6-d975-4794-9b97-682f9d6992cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first20_pred = df.sort_values([\"predictions_ridge\"],ascending=False).loc[:,['headline']][:20]\n",
    "first20_pred_causal = df.sort_values([\"causal_predictions_ridge\"],ascending=False).loc[:,['headline']][:20]\n",
    "widget1 = widgets.Output()\n",
    "widget2 = widgets.Output()\n",
    "\n",
    "# render in output widgets\n",
    "with widget1:\n",
    "    display.display(first20_pred.style.set_caption('First 20 Ridge'))\n",
    "    first20_pred.info()\n",
    "with widget2:\n",
    "    display.display(first20_pred_causal.style.set_caption('First 20 Causal'))\n",
    "    first20_pred_causal.info()\n",
    "\n",
    "\n",
    "# add some CSS styles to distribute free space\n",
    "box_layout = Layout(display='flex',\n",
    "                    flex_flow='row',\n",
    "                    justify_content='space-around',\n",
    "                    width='auto'\n",
    "                   )\n",
    "    \n",
    "\n",
    "box = widgets.HBox([widget1, widget2], layout=box_layout)\n",
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c60917-7798-4431-9495-fa4257a252f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.intersect1d(first20_pred.values,first20_pred_causal.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55adabeb-a994-4d4c-8be8-055874fc5828",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"predictions_linear\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a304c6-6c63-44b2-8678-3ff7b28a70cf",
   "metadata": {},
   "source": [
    "## 3.1 Compare top / bottom 20 causal model with correlational model - Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac03f733-144e-44d6-b33b-b7ab87c4d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "last20_pred = df.sort_values([\"predictions_linear\"],ascending=True).loc[:,['headline']][:20]\n",
    "last20_pred_causal = df.sort_values([\"causal_predictions_linear\"],ascending=True).loc[:,['headline']][:20]\n",
    "widget1 = widgets.Output()\n",
    "widget2 = widgets.Output()\n",
    "\n",
    "# render in output widgets\n",
    "with widget1:\n",
    "    display.display(last20_pred.style.set_caption('Last 20 Linear'))\n",
    "    last20_pred.info()\n",
    "with widget2:\n",
    "    display.display(last20_pred_causal.style.set_caption('Last 20 Causal'))\n",
    "    last20_pred_causal.info()\n",
    "\n",
    "\n",
    "# add some CSS styles to distribute free space\n",
    "box_layout = Layout(display='flex',\n",
    "                    flex_flow='row',\n",
    "                    justify_content='space-around',\n",
    "                    width='auto'\n",
    "                   )\n",
    "    \n",
    "\n",
    "box = widgets.HBox([widget1, widget2], layout=box_layout)\n",
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b296bb-cef5-487f-92bf-d0878546b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for intersection\n",
    "np.intersect1d(last20_pred.values,last20_pred_causal.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a1a6a7-e4ee-47a7-8f24-04fb1d9fe41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "first20_pred = df.sort_values([\"predictions_linear\"],ascending=False).loc[:,['headline']][:20]\n",
    "first20_pred_causal = df.sort_values([\"causal_predictions_linear\"],ascending=False).loc[:,['headline']][:20]\n",
    "widget1 = widgets.Output()\n",
    "widget2 = widgets.Output()\n",
    "\n",
    "# render in output widgets\n",
    "with widget1:\n",
    "    display.display(first20_pred.style.set_caption('First 20 Linear'))\n",
    "    first20_pred.info()\n",
    "with widget2:\n",
    "    display.display(first20_pred_causal.style.set_caption('First 20 Causal'))\n",
    "    first20_pred_causal.info()\n",
    "\n",
    "\n",
    "# add some CSS styles to distribute free space\n",
    "box_layout = Layout(display='flex',\n",
    "                    flex_flow='row',\n",
    "                    justify_content='space-around',\n",
    "                    width='auto'\n",
    "                   )\n",
    "    \n",
    "\n",
    "box = widgets.HBox([widget1, widget2], layout=box_layout)\n",
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf376c6-8a59-4949-ad7b-36debe44db3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for intersection\n",
    "np.intersect1d(first20_pred.values,first20_pred_causal.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5609d4-b2cd-423c-8c3f-2bc2c769add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from hdbscan import HDBSCAN\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1,2),\n",
    "                                   stop_words=nlp.Defaults.stop_words)\n",
    "\n",
    "cluster_model = HDBSCAN(min_cluster_size=300, min_samples=5, metric='euclidean', prediction_data = True)\n",
    "\n",
    "topic_model = BERTopic(embedding_model=model, language='English',\n",
    "                       verbose=True,calculate_probabilities=False,\n",
    "                       vectorizer_model = vectorizer_model,\n",
    "                       n_gram_range=(1,2), min_topic_size = 100,\n",
    "                       hdbscan_model=cluster_model, diversity=0.3)\n",
    "\n",
    "topic, probs = topic_model.fit_transform(headlines, stored_embeddings)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
