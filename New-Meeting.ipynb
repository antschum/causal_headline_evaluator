{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a712caa8",
   "metadata": {},
   "source": [
    "2023-12-12 Meeting\n",
    "\n",
    "- Use click rate everywhere\n",
    "- Implement Causal Model: + Reading\n",
    "    - We divide every click rate by the article mean and the plug it in regression\n",
    "- Make Regplot https://seaborn.pydata.org/generated/seaborn.regplot.html - should expect a clear upwards trend\n",
    "- Taking the predictions of the correlational model and compare the top and least rated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a417d79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeCV, LogisticRegression, LinearRegression\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import csv\n",
    "import torch \n",
    "import pickle\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import spearmanr\n",
    "import math\n",
    "import shap\n",
    "#import langid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0395e4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "#cpu/gpu\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Set random seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70b7888d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72230, 10)\n",
      "we removed:  21538\n",
      "         clickability_test_id             eyecatcher_id  \\\n",
      "30   546de9399ad54eca4800003c  546de0d084ad380b59000031   \n",
      "31   546de9399ad54eca4800003c  546de0d084ad380b59000031   \n",
      "32   546de9399ad54eca4800003c  546de0d084ad380b59000031   \n",
      "33   546de9399ad54eca4800003c  546de0d084ad380b59000031   \n",
      "34   546de9399ad54eca4800003c  546de0d084ad380b59000031   \n",
      "40   546deed784ad3817bc000057  546de75726714c6c44000041   \n",
      "43   546deed784ad3817bc000057  546de75726714c6c44000041   \n",
      "44   546deed784ad3817bc000057  546de75726714c6c44000041   \n",
      "45   546deed784ad3817bc000057  546de75726714c6c44000041   \n",
      "49   546df43d84ad385a0a000042  546de0d084ad380b59000031   \n",
      "50   546df43d84ad385a0a000042  546de0d084ad380b59000031   \n",
      "51   546df43d84ad385a0a000042  546de0d084ad380b59000031   \n",
      "53   546df43d84ad385a0a000042  546de0d084ad380b59000031   \n",
      "54   546df43d84ad385a0a000042  546de0d084ad380b59000031   \n",
      "55   546df43d84ad385a0a000042  546de0d084ad380b59000031   \n",
      "128  546e39c684ad380752000071  546e1d5684ad381bbe00004c   \n",
      "129  546e39c684ad380752000071  546e1d5684ad381bbe00004c   \n",
      "130  546e39c684ad380752000071  546e1d5684ad381bbe00004c   \n",
      "131  546e39c684ad380752000071  546e1d5684ad381bbe00004c   \n",
      "144  546eb48d92f391daa3000011  546e898192f391552c000005   \n",
      "\n",
      "                                              headline  \n",
      "30   Sometimes Innocent People Are Executed. That's...  \n",
      "31   Sometimes Innocent People Are Executed. That's...  \n",
      "32   Sometimes Innocent People Are Executed. That's...  \n",
      "33   Sometimes Innocent People Are Executed. That's...  \n",
      "34   Sometimes Innocent People Are Executed. That's...  \n",
      "40   A Bunch Of Celebrities Recorded A Song To Rais...  \n",
      "43   A Bunch Of Celebrities Recorded A Song To Rais...  \n",
      "44   A Bunch Of Celebrities Recorded A Song To Rais...  \n",
      "45   A Bunch Of Celebrities Recorded A Song To Rais...  \n",
      "49   Sometimes Innocent People Are Executed. That's...  \n",
      "50   Sometimes Innocent People Are Executed. That's...  \n",
      "51   Sometimes Innocent People Are Executed. That's...  \n",
      "53   Sometimes Innocent People Are Executed. That's...  \n",
      "54   Sometimes Innocent People Are Executed. That's...  \n",
      "55   Sometimes Innocent People Are Executed. That's...  \n",
      "128  The Selfies Were Already Bad Enough, But Then ...  \n",
      "129  The Selfies Were Already Bad Enough, But Then ...  \n",
      "130  The Selfies Were Already Bad Enough, But Then ...  \n",
      "131  The Selfies Were Already Bad Enough, But Then ...  \n",
      "144  10 Remarkable Things About Lucille Ball That M...  \n",
      "(50692, 10)\n",
      "Empty DataFrame\n",
      "Columns: [clickability_test_id, excerpt, headline, lede, eyecatcher_id, clicks, headline_count, embedding_id, clickrate, impressions]\n",
      "Index: []\n",
      "(34323, 10)\n"
     ]
    }
   ],
   "source": [
    "#Load data\n",
    "df = pd.read_csv(\"upworthy-archive-confirmatory-packages-03.12.2020.csv\", low_memory=False)\n",
    "#adding index\n",
    "df.reset_index(inplace=True,names=[\"embedding_id\"])\n",
    "\n",
    "#remove rows without eyecatcher_id (about 100)\n",
    "has_eyecatcher_id = df['eyecatcher_id'].notna()\n",
    "df = df.loc[has_eyecatcher_id]\n",
    "#Create a new column for clickrate\n",
    "df[\"clickrate\"] = round((df[\"clicks\"]/ df[\"impressions\"]), ndigits=10)\n",
    "\n",
    "#filter data based on same clickability_id and eyecatcher_id\n",
    "df['headline_count'] = df.groupby(['clickability_test_id', 'eyecatcher_id']).headline.transform('count')\n",
    "df.columns\n",
    "# filter for all headlines with at least 2 pairs. \n",
    "df = df.loc[df['headline_count']>=2, ['clickability_test_id', 'excerpt', 'headline', 'lede', 'eyecatcher_id', 'clicks', 'headline_count',\"embedding_id\",\"clickrate\",\"impressions\"]]\n",
    "\n",
    "# drop all rows with same headline, clickability_test_id and eyecatcher_id\n",
    "\n",
    "#df = df.drop_duplicates(subset=[\"headline\",\"clickability_test_id\",\"eyecatcher_id\"],keep=False)\n",
    "\n",
    "cti = df[df.duplicated(subset=[\"headline\",\"clickability_test_id\",\"eyecatcher_id\"],keep=False)].clickability_test_id\n",
    "eti = df[df.duplicated(subset=[\"headline\",\"clickability_test_id\",\"eyecatcher_id\"],keep=False)].eyecatcher_id\n",
    "print(df.shape)\n",
    "print(\"we removed: \",((df['clickability_test_id'].isin(cti) & df['eyecatcher_id'].isin(eti))).sum())\n",
    "print(df[((df['clickability_test_id'].isin(cti) & df['eyecatcher_id'].isin(eti)))][['clickability_test_id', 'eyecatcher_id', 'headline']][:20])\n",
    "df = df[~(df['clickability_test_id'].isin(cti) & df['eyecatcher_id'].isin(eti))]\n",
    "print(df.shape)\n",
    "\n",
    "#checking if it was successful\n",
    "# -> could there be cases where there are duplicates and we only delete the headlines and not the whole experiment?\n",
    "print(df[df[\"clickability_test_id\"] == \"546de9399ad54eca4800003c\"]) #this is an example of matching headline, clickability test id and eyecatcher id\n",
    "df = df.sort_values(by='headline_count', ascending=False)\n",
    "#print(df.head())\n",
    "#checking if there are any duplicates\n",
    "#df = df.drop_duplicates(subset=[\"headline\"]) ##this was before\n",
    "##this is a new version\n",
    "dupl_headline = df[df.duplicated(subset=[\"headline\"])] \n",
    "## I believe we are removing too many values here - one of the duplicates we want to keep right?\n",
    "\n",
    "#duplicated = df[dupl_headline]\n",
    "ids = dupl_headline[\"clickability_test_id\"]\n",
    "eid = dupl_headline[\"eyecatcher_id\"]\n",
    "\n",
    "\n",
    "mask = df['clickability_test_id'].isin(ids)&df['eyecatcher_id'].isin(eid)\n",
    "df = df[~mask]\n",
    "print(df.shape) #too few observations :(\n",
    "#print(df[df[\"clickability_test_id\"] == \"545181f8763e26efef000001\"]) #those are staying because there are a lot of observations\n",
    "#print(df[df[\"clickability_test_id\"] == \"54518b6da54be28ef000000b\"]) #those are not staying because there a less packages within the test than then in the one above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8406f0d9-d5cb-4a41-9ee2-0ef7f7a8f7d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clickability_test_id</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>headline</th>\n",
       "      <th>lede</th>\n",
       "      <th>eyecatcher_id</th>\n",
       "      <th>clicks</th>\n",
       "      <th>headline_count</th>\n",
       "      <th>embedding_id</th>\n",
       "      <th>clickrate</th>\n",
       "      <th>impressions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [clickability_test_id, excerpt, headline, lede, eyecatcher_id, clicks, headline_count, embedding_id, clickrate, impressions]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that we remove all cases within experiment. \n",
    "df[(df['clickability_test_id']=='54ad5a2e65343000152d0000')\n",
    "&(df['eyecatcher_id']=='54ad4e7b653430001e1c0000')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eba46a9b-bd0b-4b68-98da-d81df4eec9d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that there are no more duplicate headlines. \n",
    "len(df.headline.drop_duplicates()) == len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f4a326a-a46e-4cc2-9d8f-c74b96682869",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'langid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Checking for spanish headlines\u001b[39;00m\n\u001b[1;32m      2\u001b[0m headlines \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheadline\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m----> 3\u001b[0m results \u001b[38;5;241m=\u001b[39m [langid\u001b[38;5;241m.\u001b[39mclassify(headline) \u001b[38;5;28;01mfor\u001b[39;00m headline \u001b[38;5;129;01min\u001b[39;00m headlines]\n\u001b[1;32m      4\u001b[0m log_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_log.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(log_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m log:\n",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Checking for spanish headlines\u001b[39;00m\n\u001b[1;32m      2\u001b[0m headlines \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheadline\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m----> 3\u001b[0m results \u001b[38;5;241m=\u001b[39m [\u001b[43mlangid\u001b[49m\u001b[38;5;241m.\u001b[39mclassify(headline) \u001b[38;5;28;01mfor\u001b[39;00m headline \u001b[38;5;129;01min\u001b[39;00m headlines]\n\u001b[1;32m      4\u001b[0m log_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_log.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(log_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m log:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'langid' is not defined"
     ]
    }
   ],
   "source": [
    "#Checking for spanish headlines\n",
    "headlines = df['headline'].astype(str).tolist()\n",
    "results = [langid.classify(headline) for headline in headlines]\n",
    "log_file = \"output_log.txt\"\n",
    "\n",
    "with open(log_file, \"w\", encoding=\"utf-8\") as log:\n",
    "    for headline,result in zip(headlines,results):\n",
    "        language = result\n",
    "       \n",
    "        log.write(f\"Sentence: {headline}, Identified Language: {language}\\n\")\n",
    "#doesnt work amazingly for some reason and it just outputs all the sentences. The spanish ones are:\n",
    "#Como Decir Todo … Sin Pronunciar Ninguna Palabra\n",
    "#Ve La Protesta Que Todos Deben Conocer, Pero Que Nadie Puede Oir\n",
    "#En Vez De ‘Sí Se Puede,’ Ya Es ‘Sí Se Shhhhhhhhh’?\n",
    "#¿Cómo Se Dice ‘Nada’ En Español?\n",
    "#all of them have the same clickability_id, so I would just remove them, because they are basically one experiment\n",
    "print(df.shape)\n",
    "df = df[df[\"clickability_test_id\"] != \"51436075220cb800020007b3\"]\n",
    "#checking if i dropped exactly 4\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2e9301b-1bc9-439c-9284-fdb14133e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tensor\n",
    "clickrate = torch.tensor(df.clickrate.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e091a221-d671-4582-9799-a15d5a8870c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excerpt           4.007623\n",
      "headline          4.590477\n",
      "lede              4.587134\n",
      "eyecatcher_id     4.590477\n",
      "clicks            4.590477\n",
      "headline_count    4.590477\n",
      "embedding_id      4.590477\n",
      "clickrate         4.590477\n",
      "impressions       4.590477\n",
      "dtype: float64\n",
      "excerpt           3.981531\n",
      "headline          4.560590\n",
      "lede              4.557268\n",
      "clicks            4.560590\n",
      "headline_count    4.560590\n",
      "embedding_id      4.560590\n",
      "clickrate         4.560590\n",
      "impressions       4.560590\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clickability_test_id</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>headline</th>\n",
       "      <th>lede</th>\n",
       "      <th>eyecatcher_id</th>\n",
       "      <th>clicks</th>\n",
       "      <th>headline_count</th>\n",
       "      <th>embedding_id</th>\n",
       "      <th>clickrate</th>\n",
       "      <th>impressions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82811</th>\n",
       "      <td>541dd09b88aa4b01c3000001</td>\n",
       "      <td>Things that matter. Pass 'em on.</td>\n",
       "      <td>Earth’s Final Message To Earth Is Actually Rea...</td>\n",
       "      <td>&lt;p&gt;LIGHTBULB MOMENT: I think I figured out wha...</td>\n",
       "      <td>541dd246f509e102bb000005</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>82811</td>\n",
       "      <td>0.003135</td>\n",
       "      <td>4147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82805</th>\n",
       "      <td>541dd09b88aa4b01c3000001</td>\n",
       "      <td>Things that matter. Pass 'em on.</td>\n",
       "      <td>A PSA So Funny And Weird It Might Actually Get...</td>\n",
       "      <td>&lt;p&gt;This is the first 'save the Earth' PSA (pla...</td>\n",
       "      <td>541dd246f509e102bb000005</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>82805</td>\n",
       "      <td>0.003992</td>\n",
       "      <td>4008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63587</th>\n",
       "      <td>534a0b4e55c9d331f2000019</td>\n",
       "      <td>Things that matter. Pass 'em on. #PromotedPost</td>\n",
       "      <td>Watch This Fantastically Cute And Uplifting Vi...</td>\n",
       "      <td>&lt;p&gt;The world can be a tough place sometimes fo...</td>\n",
       "      <td>5349d78bcef1f1e528000017</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>63587</td>\n",
       "      <td>0.011262</td>\n",
       "      <td>2131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62987</th>\n",
       "      <td>534a0b4e55c9d331f2000019</td>\n",
       "      <td>Things that matter. Pass 'em on. #PromotedPost</td>\n",
       "      <td>Watch This Ridiculously Cute And Uplifting Vid...</td>\n",
       "      <td>&lt;p&gt;The world can be a tough place sometimes fo...</td>\n",
       "      <td>5349d78bcef1f1e528000017</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>62987</td>\n",
       "      <td>0.011241</td>\n",
       "      <td>2135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14325</th>\n",
       "      <td>534a0b4e55c9d331f2000019</td>\n",
       "      <td>Things that matter. Pass 'em on. #PromotedPost</td>\n",
       "      <td>Watch This Ridiculously Cute And Uplifting Vid...</td>\n",
       "      <td>&lt;p&gt;The world can be a tough place sometimes fo...</td>\n",
       "      <td>5349d78bcef1f1e528000017</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>14325</td>\n",
       "      <td>0.011289</td>\n",
       "      <td>2126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           clickability_test_id  \\\n",
       "82811  541dd09b88aa4b01c3000001   \n",
       "82805  541dd09b88aa4b01c3000001   \n",
       "63587  534a0b4e55c9d331f2000019   \n",
       "62987  534a0b4e55c9d331f2000019   \n",
       "14325  534a0b4e55c9d331f2000019   \n",
       "\n",
       "                                              excerpt  \\\n",
       "82811                Things that matter. Pass 'em on.   \n",
       "82805                Things that matter. Pass 'em on.   \n",
       "63587  Things that matter. Pass 'em on. #PromotedPost   \n",
       "62987  Things that matter. Pass 'em on. #PromotedPost   \n",
       "14325  Things that matter. Pass 'em on. #PromotedPost   \n",
       "\n",
       "                                                headline  \\\n",
       "82811  Earth’s Final Message To Earth Is Actually Rea...   \n",
       "82805  A PSA So Funny And Weird It Might Actually Get...   \n",
       "63587  Watch This Fantastically Cute And Uplifting Vi...   \n",
       "62987  Watch This Ridiculously Cute And Uplifting Vid...   \n",
       "14325  Watch This Ridiculously Cute And Uplifting Vid...   \n",
       "\n",
       "                                                    lede  \\\n",
       "82811  <p>LIGHTBULB MOMENT: I think I figured out wha...   \n",
       "82805  <p>This is the first 'save the Earth' PSA (pla...   \n",
       "63587  <p>The world can be a tough place sometimes fo...   \n",
       "62987  <p>The world can be a tough place sometimes fo...   \n",
       "14325  <p>The world can be a tough place sometimes fo...   \n",
       "\n",
       "                  eyecatcher_id  clicks  headline_count  embedding_id  \\\n",
       "82811  541dd246f509e102bb000005      13              17         82811   \n",
       "82805  541dd246f509e102bb000005      16              17         82805   \n",
       "63587  5349d78bcef1f1e528000017      24              17         63587   \n",
       "62987  5349d78bcef1f1e528000017      24              17         62987   \n",
       "14325  5349d78bcef1f1e528000017      24              17         14325   \n",
       "\n",
       "       clickrate  impressions  \n",
       "82811   0.003135         4147  \n",
       "82805   0.003992         4008  \n",
       "63587   0.011262         2131  \n",
       "62987   0.011241         2135  \n",
       "14325   0.011289         2126  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.groupby([\"clickability_test_id\"]).count().mean()) #average of 3.5 packages within one test\n",
    "print(df.groupby([\"clickability_test_id\",\"eyecatcher_id\"]).count().mean()) #average of 3.5 packages with the same eyecatcher id and same clickability_test_id\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "137ce53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load prerun embeddings of all-mpnet-base-v2\n",
    "with open('full-all-mpnet-base-v2_embeddings.pkl', \"rb\") as fIn:\n",
    "    stored_data = pickle.load(fIn)\n",
    "    stored_sentences = stored_data['headlines']\n",
    "    stored_embeddings = stored_data['embeddings']\n",
    "#embeddings = [stored_embeddings[stored_sentences.index.get_loc(headline)] for headline in df.headline]\n",
    "#make sure we are getting the right embeddings: \n",
    "\n",
    "    \n",
    "#remove rows without eyecatcher_id\n",
    "#stored_sentences = stored_sentences[has_eyecatcher_id]\n",
    "#stored_embeddings = stored_embeddings[has_eyecatcher_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f76e17",
   "metadata": {},
   "source": [
    "## 1.1 Predicting clickrate from headline embeddings with Ridge regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e082b375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([105551, 768])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [105551, 49357]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(stored_embeddings\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 3\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstored_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclickrate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2614\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2611\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2612\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2614\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2616\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   2617\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2618\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[1;32m   2619\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/sklearn/utils/validation.py:455\u001b[0m, in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \n\u001b[1;32m    438\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    454\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[0;32m--> 455\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/sklearn/utils/validation.py:409\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    407\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    412\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [105551, 49357]"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print(stored_embeddings.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(stored_embeddings, clickrate, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8912d9b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Ridge Model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m ridge_model \u001b[38;5;241m=\u001b[39mRidgeCV(alphas\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.001\u001b[39m,\u001b[38;5;241m0.002\u001b[39m,\u001b[38;5;241m0.005\u001b[39m,\u001b[38;5;241m0.01\u001b[39m,\u001b[38;5;241m0.05\u001b[39m,\u001b[38;5;241m0.07\u001b[39m,\u001b[38;5;241m0.2\u001b[39m,\u001b[38;5;241m0.4\u001b[39m,\u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m],store_cv_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m ridge_model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[1;32m      4\u001b[0m ridge_model\u001b[38;5;241m.\u001b[39mscore(X_train,y_train)\n\u001b[1;32m      5\u001b[0m predictions \u001b[38;5;241m=\u001b[39m ridge_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Ridge Model\n",
    "ridge_model =RidgeCV(alphas=[0.001,0.002,0.005,0.01,0.05,0.07,0.2,0.4,0.6, 1, 10],store_cv_values=True)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "ridge_model.score(X_train,y_train)\n",
    "predictions = ridge_model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, predictions)\n",
    "print(\"Ridge Regression MSE for click difference:\", rmse)\n",
    "print(\"Ridge Regression R2 for click difference:\", r2_score(y_true=y_test, y_pred=predictions))\n",
    "df[\"predictions_ridge\"] = ridge_model.predict(stored_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f807f1",
   "metadata": {},
   "source": [
    "## 1.2 Predicting clickrate from headline embeddings with Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9503b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Model\n",
    "linear_model =LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "linear_model.score(X_train,y_train)\n",
    "predictions = linear_model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "print(\"Linear Regression MSE for clicks:\", rmse)\n",
    "print(\"Linear Regression R2 for clicks:\", r2_score(y_true=y_test, y_pred=predictions))\n",
    "df[\"predictions_linear\"] = linear_model.predict(stored_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021623a7",
   "metadata": {},
   "source": [
    "## 1.3 Visualizing predicted clicks vs actual clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5e5243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\n",
    "# print stuff. \n",
    "predictions = ridge_model.predict(stored_embeddings)\n",
    "print(predictions.max())\n",
    "# visualize real and predicted values\n",
    "fig, ax = plt.subplots()\n",
    "sns.regplot(x = predictions, y = df.clickrate, ax=ax)\n",
    "ax.set(ylabel = 'true clickrate', xlabel = 'predicted clickrate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1711e851",
   "metadata": {},
   "source": [
    "## 2.Causal Model \n",
    "\n",
    "Implement Causal Model: + Reading\n",
    "    - We divide every click rate by the article mean and the plug it in regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73398aa-4ade-4412-ab41-d2e25ea0e1ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calculating mean per clickability_test_id and eyecatcher_id\n",
    "df[\"means\"] = df.groupby([\"clickability_test_id\",\"eyecatcher_id\"])[\"clickrate\"].transform(\"mean\")\n",
    "df.columns\n",
    "df[\"adjusted_clickrate\"] = df[\"clickrate\"]/df[\"means\"]\n",
    "# set rows with 0 clicks -> adjusted clickrates NA - set to zero\n",
    "df[\"adjusted_clickrate\"] = df[\"adjusted_clickrate\"].fillna(0)\n",
    "print(df[\"adjusted_clickrate\"].isna().sum())\n",
    "adjusted_clickrate = torch.tensor(df.adjusted_clickrate.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67466e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do we do with headlines that did not get any clicks? \n",
    "# Will they just have an adjusted clickrate of zero?\n",
    "df.loc[(df[\"clickrate\"]/df[\"means\"]).isna(), ['headline', 'eyecatcher_id', 'clicks', 'impressions', 'means', 'clickrate']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0254cb-af0d-4d64-93c0-3106c0152430",
   "metadata": {},
   "source": [
    "## 2.1 Causal model with Ridge Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4c0990-b442-4b5f-b011-871f731f8194",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(stored_embeddings, adjusted_clickrate, test_size=0.2)\n",
    "causal_ridge_model =RidgeCV(alphas=[0.001,0.002,0.005,0.01,0.05,0.07,0.2,0.4,0.6, 1, 10],store_cv_values=True)\n",
    "causal_ridge_model.fit(X_train, y_train)\n",
    "causal_ridge_model.score(X_train,y_train)\n",
    "causal_predictions_rg = causal_ridge_model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, causal_predictions_rg)\n",
    "print(\"Causal Ridge Regression MSE for click difference:\", rmse)\n",
    "print(\"Causal Ridge Regression R2 for click difference:\", r2_score(y_true=y_test, y_pred=causal_predictions_rg))\n",
    "df[\"causal_predictions_ridge\"] = causal_ridge_model.predict(stored_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17d5407-c759-4ea3-acd3-eb8afae04ee2",
   "metadata": {},
   "source": [
    "## 2.2 Causal model with Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7635f29-4864-4141-a142-9026c06ba913",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_linear_model =LinearRegression()\n",
    "causal_linear_model.fit(X_train, y_train)\n",
    "causal_linear_model.score(X_train,y_train)\n",
    "causal_predictions_lm = causal_linear_model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, causal_predictions_lm, squared=False)\n",
    "print(\"Causal Linear Regression MSE for clicks:\", rmse)\n",
    "print(\"Causal Linear Regression R2 for clicks:\", r2_score(y_true=y_test, y_pred=causal_predictions_lm))\n",
    "df[\"causal_predictions_linear\"] = causal_linear_model.predict(stored_embeddings)\n",
    "sns.histplot(df[\"causal_predictions_linear\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759f696b",
   "metadata": {},
   "source": [
    "## 3.1 Compare top / bottom 20 causal model with correlational model - Ridge Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dd3c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets, Layout\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "last20_pred = df.sort_values([\"predictions_ridge\"],ascending=True).loc[:,['headline']][:20]\n",
    "last20_pred_causal = df.sort_values([\"causal_predictions_ridge\"],ascending=True).loc[:,['headline']][:20]\n",
    "widget1 = widgets.Output()\n",
    "widget2 = widgets.Output()\n",
    "\n",
    "# render in output widgets\n",
    "with widget1:\n",
    "    display.display(last20_pred.style.set_caption('Last 20 Ridge'))\n",
    "    last20_pred.info()\n",
    "with widget2:\n",
    "    display.display(last20_pred_causal.style.set_caption('Last 20 Causal'))\n",
    "    last20_pred_causal.info()\n",
    "\n",
    "\n",
    "# add some CSS styles to distribute free space\n",
    "box_layout = Layout(display='flex',\n",
    "                    flex_flow='row',\n",
    "                    justify_content='space-around',\n",
    "                    width='auto'\n",
    "                   )\n",
    "    \n",
    "\n",
    "box = widgets.HBox([widget1, widget2], layout=box_layout)\n",
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492a61f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for intersection\n",
    "np.intersect1d(last20_pred.values,last20_pred_causal.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321c94a6-d975-4794-9b97-682f9d6992cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first20_pred = df.sort_values([\"predictions_ridge\"],ascending=False).loc[:,['headline']][:20]\n",
    "first20_pred_causal = df.sort_values([\"causal_predictions_ridge\"],ascending=False).loc[:,['headline']][:20]\n",
    "widget1 = widgets.Output()\n",
    "widget2 = widgets.Output()\n",
    "\n",
    "# render in output widgets\n",
    "with widget1:\n",
    "    display.display(first20_pred.style.set_caption('First 20 Ridge'))\n",
    "    first20_pred.info()\n",
    "with widget2:\n",
    "    display.display(first20_pred_causal.style.set_caption('First 20 Causal'))\n",
    "    first20_pred_causal.info()\n",
    "\n",
    "\n",
    "# add some CSS styles to distribute free space\n",
    "box_layout = Layout(display='flex',\n",
    "                    flex_flow='row',\n",
    "                    justify_content='space-around',\n",
    "                    width='auto'\n",
    "                   )\n",
    "    \n",
    "\n",
    "box = widgets.HBox([widget1, widget2], layout=box_layout)\n",
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c60917-7798-4431-9495-fa4257a252f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.intersect1d(first20_pred.values,first20_pred_causal.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55adabeb-a994-4d4c-8be8-055874fc5828",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"predictions_linear\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a304c6-6c63-44b2-8678-3ff7b28a70cf",
   "metadata": {},
   "source": [
    "## 3.1 Compare top / bottom 20 causal model with correlational model - Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac03f733-144e-44d6-b33b-b7ab87c4d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "last20_pred = df.sort_values([\"predictions_linear\"],ascending=True).loc[:,['headline']][:20]\n",
    "last20_pred_causal = df.sort_values([\"causal_predictions_linear\"],ascending=True).loc[:,['headline']][:20]\n",
    "widget1 = widgets.Output()\n",
    "widget2 = widgets.Output()\n",
    "\n",
    "# render in output widgets\n",
    "with widget1:\n",
    "    display.display(last20_pred.style.set_caption('Last 20 Linear'))\n",
    "    last20_pred.info()\n",
    "with widget2:\n",
    "    display.display(last20_pred_causal.style.set_caption('Last 20 Causal'))\n",
    "    last20_pred_causal.info()\n",
    "\n",
    "\n",
    "# add some CSS styles to distribute free space\n",
    "box_layout = Layout(display='flex',\n",
    "                    flex_flow='row',\n",
    "                    justify_content='space-around',\n",
    "                    width='auto'\n",
    "                   )\n",
    "    \n",
    "\n",
    "box = widgets.HBox([widget1, widget2], layout=box_layout)\n",
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b296bb-cef5-487f-92bf-d0878546b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for intersection\n",
    "np.intersect1d(last20_pred.values,last20_pred_causal.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a1a6a7-e4ee-47a7-8f24-04fb1d9fe41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "first20_pred = df.sort_values([\"predictions_linear\"],ascending=False).loc[:,['headline']][:20]\n",
    "first20_pred_causal = df.sort_values([\"causal_predictions_linear\"],ascending=False).loc[:,['headline']][:20]\n",
    "widget1 = widgets.Output()\n",
    "widget2 = widgets.Output()\n",
    "\n",
    "# render in output widgets\n",
    "with widget1:\n",
    "    display.display(first20_pred.style.set_caption('First 20 Linear'))\n",
    "    first20_pred.info()\n",
    "with widget2:\n",
    "    display.display(first20_pred_causal.style.set_caption('First 20 Causal'))\n",
    "    first20_pred_causal.info()\n",
    "\n",
    "\n",
    "# add some CSS styles to distribute free space\n",
    "box_layout = Layout(display='flex',\n",
    "                    flex_flow='row',\n",
    "                    justify_content='space-around',\n",
    "                    width='auto'\n",
    "                   )\n",
    "    \n",
    "\n",
    "box = widgets.HBox([widget1, widget2], layout=box_layout)\n",
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf376c6-8a59-4949-ad7b-36debe44db3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for intersection\n",
    "np.intersect1d(first20_pred.values,first20_pred_causal.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b248dfc3",
   "metadata": {},
   "source": [
    "## 4.1 Shapley Values Ridge Regression predicting Clickrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c529ef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "\n",
    "# here we also need embeddings - the whole thing is our model\n",
    "model = SentenceTransformer(\n",
    "    \"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# this defines an explicit python function that takes a list of strings and outputs scores for each class\n",
    "def f(x):\n",
    "    embedding = model.encode(x)\n",
    "    return ridge_model.predict(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7d40bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining masker - the masker specifies how to hide portions of the input\n",
    "method = \"default masker\" #before it was custo tokenizer, but it works the same with the default one so i would say remove everything and just leave the defining of the whitespace masker\n",
    "\n",
    "# build an explainer by passing a transformers tokenizer\n",
    "if method == \"transformers tokenizer\":\n",
    "    explainer = shap.Explainer(f)\n",
    "\n",
    "# build an explainer by explicitly creating a masker\n",
    "elif method == \"default masker\":\n",
    "    masker  = shap.maskers.Text(\"\\W|'\")   # this will create a basic whitespace tokenizer.\n",
    "    #A whitespace tokenizer breaks a text into tokens based on whitespace characters (spaces, tabs, newlines,/, etc.)\n",
    "    explainer = shap.Explainer(f, masker)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e55b6f-0233-4b33-8054-55ef49e9a81d",
   "metadata": {},
   "source": [
    "### 4.1.1 Checking Shap values for top 20 predictions made with ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40266b3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_values = df.sort_values([\"predictions_ridge\"],ascending=False).loc[:,['headline']][:20]\n",
    "#removing duplicates, so they dont influence the overall \n",
    "top_values = top_values.drop_duplicates(keep='first')\n",
    "top_shap_values_ridge = explainer(top_values[\"headline\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac73286e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#predictions = ridge_model.predict(stored_embeddings[0].reshape(1,-1)) # i am not sure what is that and if we need it ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcd0857",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5): #just checking for 5 of them for example\n",
    "    shap.plots.text(top_shap_values_ridge[i])\n",
    "shap.plots.bar(top_shap_values_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd86713-c957-48cf-90ea-84bf5ade0462",
   "metadata": {},
   "source": [
    "### 4.1.2 Checking for least 20 if there are words that had negative impact, because they will be the ones that will make the click rate lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2226025-fee0-4670-adbc-7517b83e9ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_values = df.sort_values([\"predictions_ridge\"],ascending=True).loc[:,['headline']][:20]\n",
    "lowest_values[\"headline\"]\n",
    "lowest_values = lowest_values.drop_duplicates(keep='first')\n",
    "lowest_shap_values_ridge = explainer(lowest_values[\"headline\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a684f7d1-d32e-4c11-ac3f-71f33a8e13ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    shap.plots.text(lowest_shap_values_ridge[i])\n",
    "shap.plots.bar(lowest_shap_values_ridge.mean(0),order=shap.Explanation.argsort)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fe3a19-ea90-4020-8aa2-a05559d1d06f",
   "metadata": {},
   "source": [
    "## 4.2 Shapley Values Linear Regression predicting Clickrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272a5e83-77fa-4f7d-89d1-7973fb1d7abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "\n",
    "# here we also need embeddings - the whole thing is our model\n",
    "model = SentenceTransformer(\n",
    "    \"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# this defines an explicit python function that takes a list of strings and outputs scores for each class\n",
    "def f(x):\n",
    "    embedding = model.encode(x)\n",
    "    return linear_model.predict(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a870b0-f64f-4a4d-bc21-b7bf3176be0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining masker - the masker specifies how to hide portions of the input\n",
    "method = \"default masker\" #before it was custo tokenizer, but it works the same with the default one so i would say remove everything and just leave the defining of the whitespace masker\n",
    "\n",
    "# build an explainer by passing a transformers tokenizer\n",
    "if method == \"transformers tokenizer\":\n",
    "    explainer = shap.Explainer(f)\n",
    "\n",
    "# build an explainer by explicitly creating a masker\n",
    "elif method == \"default masker\":\n",
    "    masker  = shap.maskers.Text(\"\\W|'\")   # this will create a basic whitespace tokenizer.\n",
    "    #A whitespace tokenizer breaks a text into tokens based on whitespace characters (spaces, tabs, newlines,/, etc.)\n",
    "    explainer = shap.Explainer(f, masker)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08597adf-85aa-4275-969a-19e08f8241a0",
   "metadata": {},
   "source": [
    "### 4.2.1 Checking for shap values for top 20 predictions made with linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963e2e1a-715f-4d06-bd37-b3ed62448786",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_values = df.sort_values([\"predictions_linear\"],ascending=False).loc[:,['headline']][:20]\n",
    "#removing duplicates, so they dont influence the overall \n",
    "top_values = top_values.drop_duplicates(keep='first')\n",
    "top_shap_values_linear = explainer(top_values[\"headline\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da0f16-8b31-4683-9d20-ab95c930a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    shap.plots.text(top_shap_values_linear[i])\n",
    "shap.plots.bar(top_shap_values_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f326cd-06b1-4e7a-8015-f5d6135615cf",
   "metadata": {},
   "source": [
    "### 4.1.2 Checking for least 20 if there are words that had negative impact, because they will be the ones that will make the click rate lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78fb321-33cb-49e1-9608-d87152fa7979",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_values = df.sort_values([\"predictions_linear\"],ascending=True).loc[:,['headline']][:20]\n",
    "lowest_values = lowest_values.drop_duplicates(keep='first')\n",
    "lowest_shap_values_linear = explainer(lowest_values[\"headline\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e021cb7-9eb9-4bba-9396-54460a682b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    shap.plots.text(lowest_shap_values_linear[i])\n",
    "shap.plots.bar(lowest_shap_values_linear.mean(0),order=shap.Explanation.argsort)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
